{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66cd6760-b104-4dae-950e-27164cb4210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fbf16b0-cee7-4774-843f-320effdfef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pdb import set_trace\n",
    "\n",
    "from dataclasses import asdict\n",
    "\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset\n",
    "from lerobot.common.policies.normalize import Normalize, Unnormalize\n",
    "from lerobot.common.datasets.utils import cycle\n",
    "\n",
    "from lerobot.common.policies.act.configuration_act import ACTConfig\n",
    "from lerobot.common.policies.act.modeling_act import ACTPolicy\n",
    "\n",
    "import torch\n",
    "import multiprocessing as mp\n",
    "\n",
    "from aim import Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dd1341-9514-48b0-aae3-a84a93836cfc",
   "metadata": {},
   "source": [
    "# Set hyperparams and enable experiment tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a128a6bc-6a80-48a8-acd4-698f5970b887",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ACTConfig()\n",
    "dry_run = False\n",
    "\n",
    "if dry_run:\n",
    "    training_steps = 10\n",
    "    eval_frequency = 5\n",
    "else:\n",
    "    training_steps = 5000 * 5  # 5 was the dataloader length in ACT, one step was equivalent to training with \n",
    "                               # 40 examples with a batch size of 8\n",
    "    eval_frequency = 500 * 5\n",
    "    \n",
    "run = Run(experiment=\"transfer_cube_lerobot\", repo='dry_run' if dry_run else None)\n",
    "run[\"hparams\"] = asdict(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499ba6cb-979a-4397-8a85-a451d4b0e006",
   "metadata": {},
   "source": [
    "# Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "738528b2-ea71-438c-a2a4-4b37c3f528fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a415948ff94c158bf7abc477e8a453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 56 files:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d0e2dc95eec4272800ee6d0533c946e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 56 files:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "delta_timestamps = {\n",
    "    \"action\": [t / 50 for t in range(config.chunk_size)],  # this dataset was recorded at 50Hz\n",
    "}\n",
    "train_dataset = LeRobotDataset('lerobot/aloha_sim_transfer_cube_scripted', split='train[:80%]', delta_timestamps=delta_timestamps)\n",
    "val_dataset = LeRobotDataset('lerobot/aloha_sim_transfer_cube_scripted', split='train[80%:]', delta_timestamps=delta_timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9c4b21-ab62-46b5-a00b-78f8544ab063",
   "metadata": {},
   "source": [
    "# Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "106eb2dc-8d7a-45b6-84c3-a82e60157932",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    num_workers=mp.cpu_count()-1,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66305f5d-7ca9-44d6-99c1-ff0a487e384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    num_workers=mp.cpu_count()-1,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cba61da-0f42-40de-99f7-7ba10d01f639",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b24ca83-9464-4cd7-a4de-bb1b24947774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use imagenet stats for normalization of images\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std=[0.229, 0.224, 0.225]\n",
    "train_dataset.stats['observation.images.top']['mean'] = torch.tensor(imagenet_mean)[:, None, None]\n",
    "train_dataset.stats['observation.images.top']['std'] = torch.tensor(imagenet_std)[:, None, None]\n",
    "\n",
    "policy = ACTPolicy(config=config, dataset_stats=train_dataset.stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecd94cf2-fa63-430b-8a80-345f525dff81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 51.61M\n"
     ]
    }
   ],
   "source": [
    "n_parameters = sum(p.numel() for p in policy.parameters() if p.requires_grad)\n",
    "print(\"number of parameters: %.2fM\" % (n_parameters/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddf8a4e3-a259-403e-838d-6ffb65992174",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.cuda();\n",
    "\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f6845d-1265-4545-ac08-fdb127bfcbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 395\tTrain loss: 2.9758791923522954\r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_iter = cycle(train_dataloader)\n",
    "\n",
    "for step in range(training_steps):\n",
    "    policy.train()\n",
    "    batch = next(train_iter)\n",
    "    batch = {k: v.cuda(non_blocking=True) for k, v in batch.items()}\n",
    "    output_dict = policy.forward(batch)\n",
    "    loss = output_dict[\"loss\"]\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Step: {step}\\tTrain loss: {loss}\", end=\"\\r\")\n",
    "    run.track(loss, name='loss', step=step, context={\"subset\": \"train\"})\n",
    "\n",
    "    if (step % eval_frequency == 0 and step != 0) or step == training_steps-1:\n",
    "        policy.eval()\n",
    "        loss_cumsum = 0\n",
    "        n_examples_evaluated = 0\n",
    "        with torch.inference_mode():\n",
    "            for batch in val_dataloader:\n",
    "                output_dict = policy.forward(batch)\n",
    "                \n",
    "                loss_cumsum += output_dict[\"loss\"].item()\n",
    "                n_examples_evaluated += batch['index'].shape[0]\n",
    "\n",
    "        validation_loss = loss_cumsum / n_examples_evaluated\n",
    "        \n",
    "        print(f\"Step: {step}\\tVal loss: {loss}\")\n",
    "        run.track(validation_loss, name='loss', step=step, context={\"subset\": \"val\"})\n",
    "\n",
    "        policy.save_pretrained(f'models/ACT_{step}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caf829f-226f-4eb1-9445-27b5e574be5f",
   "metadata": {},
   "source": [
    "# Run rollout validation at end of training (simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d3caad-5609-4420-81b6-30e7c6e7aadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_aloha\n",
    "\n",
    "env = gym.make(\n",
    "    \"gym_aloha/AlohaTransferCube-v0\",\n",
    "    obs_type=\"pixels_agent_pos\",\n",
    "    max_episode_steps=500\n",
    ")\n",
    "numpy_observation, info = env.reset()\n",
    "frames = []\n",
    "frames.append(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385901ec-7b06-4141-a1cd-051415ae02c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting positions of our simulated arms\n",
    "\n",
    "import imageio\n",
    "from IPython.display import Image\n",
    "\n",
    "Image(imageio.imwrite('<bytes>', frames[0], format='PNG'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56588d8-961f-4c23-a6da-f003eaab180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/lerobot/blob/main/examples/2_evaluate_pretrained_policy.py\n",
    "\n",
    "step = 0\n",
    "done = False\n",
    "rewards, frames = [], []\n",
    "while not done:\n",
    "    state = torch.from_numpy(numpy_observation[\"agent_pos\"])\n",
    "    image = torch.from_numpy(numpy_observation[\"pixels\"]['top'])\n",
    "\n",
    "    # Convert to float32 with image from channel first in [0,255]\n",
    "    # to channel last in [0,1]\n",
    "    state = state.to(torch.float32)\n",
    "    image = image.to(torch.float32) / 255\n",
    "    image = image.permute(2, 0, 1)\n",
    "\n",
    "    # Send data tensors from CPU to GPU\n",
    "    state = state.to('cuda', non_blocking=True)\n",
    "    image = image.to('cuda', non_blocking=True)\n",
    "\n",
    "    # Add extra (empty) batch dimension, required to forward the policy\n",
    "    state = state.unsqueeze(0)\n",
    "    image = image.unsqueeze(0)\n",
    "\n",
    "    # Create the policy input dictionary\n",
    "    observation = {\n",
    "        \"observation.state\": state,\n",
    "        \"observation.images.top\": image,\n",
    "    }\n",
    "\n",
    "    observation = normalize_inputs(observation)\n",
    "\n",
    "    # Predict the next action with respect to the current observation\n",
    "    with torch.inference_mode():\n",
    "        action = policy.select_action(observation)\n",
    "        action = unnormalize_outputs({'action': action})['action']\n",
    "    \n",
    "    # Prepare the action for the environment\n",
    "    numpy_action = action.squeeze(0).to(\"cpu\").numpy()\n",
    "\n",
    "    # Step through the environment and receive a new observation\n",
    "    numpy_observation, reward, terminated, truncated, info = env.step(numpy_action)\n",
    "    print(f\"{step=} {reward=} {terminated=}\")\n",
    "\n",
    "    # Keep track of all the rewards and frames\n",
    "    rewards.append(reward)\n",
    "    frames.append(env.render())\n",
    "\n",
    "    # The rollout is considered done when the success state is reach (i.e. terminated is True),\n",
    "    # or the maximum number of iterations is reached (i.e. truncated is True)\n",
    "    done = terminated | truncated | done\n",
    "    step += 1\n",
    "\n",
    "if terminated:\n",
    "    print(\"Success!\")\n",
    "else:\n",
    "    print(\"Failure!\")\n",
    "\n",
    "# Get the speed of environment (i.e. its number of frames per second).\n",
    "fps = env.metadata[\"render_fps\"]\n",
    "\n",
    "# Encode all frames into a mp4 video.\n",
    "video_file_name = \"rollout_LeRobot_only.mp4\"\n",
    "imageio.mimsave(video_file_name, frames, fps=fps)\n",
    "\n",
    "print(f\"Video of the evaluation is available in '{video_file_name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09937c12-802d-4ca7-b25b-10aaa8313a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "video = Video(video_file_name, width=320, height=240)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3490132e-e6f2-4d8b-9cae-b83574af5dfe",
   "metadata": {},
   "source": [
    "With training, validation loss -- just like with any other type of DL model -- becomes an imprecise descriptor of the quality of learned model. Possibly in robotics to an even greater degree than in CV or NLP.\n",
    "\n",
    "This notebook is for demonstration purposes only, but to monitor training it might be worthwile to run rollout validation regularly doing trianing and to do multiple rollouts per single validation run (rollouts begin with randomized position of the cube and measuring success rate such as x successes from 100 rollouts is a very useful piece of information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52f0145-a6f4-44d9-ad0d-62aa88f49d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.track(terminated, name='success')\n",
    "run.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
